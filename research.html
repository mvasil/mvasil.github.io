<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="preload" href="https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjx4wXiWtFCc.woff2" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwiPGQ3q5d0.woff2" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAUi-qNiXg7eU0.woff2" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_Gwftx9897g.woff2" as="font" type="font/woff2" crossorigin>

    <meta charset="UTF-8">
    <meta name="author" content="Mariya I. Vasileva">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Mariya I. Vasileva</title>
    <link rel="stylesheet" href="stylesheets.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>
<body>

    <nav class="top-nav">
        <ul>
            <li class="nav-item"><a href="index.html">About</a></li>
            <li class="nav-item active"><a href="research.html">Research</a></li>
            <li class="nav-item"><a href="blog.html">Blog</a></li>
            <li class="nav-item"><a href="cinematography.html">Cinematography</a></li>
            <li class="nav-item"><a href="data/cv.pdf" target="_blank">CV</a></li>
            <li class="nav-item"><a href="contact.html">Contact</a></li>
        </ul>
    </nav>

    <table style="width:100%;max-width:800px;margin:auto;border-collapse:separate;border-spacing:0;">
        <tbody>
            <tr>
                <td>

                    <!-- Research section -->

                    <table style="width:100%;margin:auto;border-collapse:separate;border-spacing:0;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <h2>Research</h2>
                                    <p>
                                        My experience lies at the crossover between generative models, multimodal learning, vision and language, foundation models, safety and alignment.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Polyvore Outfits Dataset section -->

                    <!-- <table style="width:100%;margin:auto;border-collapse:separate;border-spacing:0;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle;text-align:center;">
                                    <div style="width:160px;height:120px;border:1px solid #ddd;display:inline-flex;align-items:center;justify-content:center;color:#888;font-size:14px;margin:auto;">
                                        Dataset
                                    </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="polyvore.html">
                                        <span class="papertitle">Polyvore Outfits Dataset</span>
                                    </a>
                                    <br>
                                    <strong>Mariya I. Vasileva</strong>,
                                    <a href="https://bryanplummer.com/">Bryan A. Plummer</a>,
                                    <a href="http://luthuli.cs.uiuc.edu/~daf/">David A. Forsyth</a>
                                    <br>
                                    <em>Large-scale fashion compatibility and visual reasoning dataset</em>
                                    <br>
                                    <p></p>
                                    <p>
                                        A large-scale dataset of fashion outfits constructed from real user-generated content on Polyvore, designed to support research in visual compatibility, retrieval, similarity learning, and multimodal reasoning. This dataset has been used extensively in subsequent work on fashion understanding and recommendation, but is often incorrectly cited or attributed. This page serves as the canonical reference.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table> -->

                    <!-- HandsOff project section -->

                    <table style="width:100%;margin:auto;border-collapse:separate;border-spacing:0;">
                        <tbody>
                            <tr onmouseout="handsoff_stop()" onmouseover="handsoff_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <div class="two" id='handsoff_image' style="opacity: 0;">
                                            <img src='images/handsoff1.png' width="160" alt="HandsOff image 1">
                                        </div>
                                        <img src='images/handsoff2.png' width="160" alt="HandsOff image 2">
                                    </div>
                                    <script type="text/javascript">
                                        function handsoff_start() {
                                            document.getElementById('handsoff_image').style.opacity = "1";
                                        }
                                        function handsoff_stop() {
                                            document.getElementById('handsoff_image').style.opacity = "0";
                                        }
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://austinxu87.github.io/handsoff/">
                                        <span class="papertitle">HandsOff: Labeled Dataset Generation with No Additional Human Annotations</span>
                                    </a>
                                    <br>
                                    <a href="https://austinxu87.github.io/">Austin Xu</a>,
                                    <strong>Mariya I. Vasileva </strong>,
                                    <a href="http://www.achaldave.com/">Achal Dave</a>,
                                    <a href="https://arjunsesh.github.io/">Arjun Seshadri</a>
                                    <br>
                                    <em>CVPR</em>, 2023 &nbsp <font color=#FF4F29><strong>(Highlight)</strong></font>
                                    <br>
                                    <a href="https://austinxu87.github.io/handsoff/">project page</a> /
                                    <a href="https://github.com/austinxu87/handsoff/">code</a> /
                                    <a href="https://arxiv.org/pdf/2212.12645.pdf">arXiv</a>
                                    <p></p>
                                    <p>
                                        Recent work leverages the expressive power of generative adversarial networks (GANs) to generate labeled synthetic datasets. These dataset generation methods often require new annotations of synthetic images, which forces practitioners to seek out annotators, curate a set of synthetic images, and ensure the quality of generated labels. We introduce the HandsOff framework, a technique capable of producing an unlimited number of synthetic images and corresponding labels after being trained on less than 50 pre-existing labeled images. Our framework avoids the practical drawbacks of prior work by unifying the field of GAN inversion with dataset generation. We generate datasets with rich pixel-wise labels in multiple challenging domains such as faces, cars, full-body human poses, and urban driving scenes. Our method achieves state-of-the-art performance in semantic segmentation, keypoint detection, and depth estimation compared to prior dataset generation approaches and transfer learning baselines. We additionally showcase its ability to address broad challenges in model development which stem from fixed, hand-annotated datasets, such as the long-tail problem in semantic segmentation.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Why Do These Match project section -->

                    <table style="width:100%;margin:auto;border-collapse:separate;border-spacing:0;">
                        <tbody>
                            <tr onmouseout="match_stop()" onmouseover="match_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <div class="two" id='match_image' style="opacity: 0;">
                                            <img src='images/match1.png' width="160" alt="match image 1">
                                        </div>
                                        <img src='images/match2.png' width="160" alt="match image 2">
                                    </div>
                                    <script type="text/javascript">
                                        function match_start() {
                                            document.getElementById('match_image').style.opacity = "1";
                                        }
                                        function match_stop() {
                                            document.getElementById('match_image').style.opacity = "0";
                                        }
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/pdf/1905.10797">
                                        <span class="papertitle">
                                            Why do These Match? Explaining the Behavior of Image Similarity Models
                                        </span>
                                    </a>
                                    <br>
                                    <a href="https://bryanplummer.com/">Bryan A. Plummer</a>*,
                                    <strong>Mariya I. Vasileva</strong>*,
                                    <a>Vitali Petsiuk</a>,
                                    <a href="https://ai.bu.edu/ksaenko.html">Kate Saenko</a>,
                                    <a href="http://luthuli.cs.uiuc.edu/~daf/">David A. Forsyth</a>
                                    <br>
                                    <em>ECCV</em>, 2020
                                    <br>
                                    <a href="https://github.com/VisionLearningGroup/SANE">code</a> /
                                    <a href="https://arxiv.org/abs/1905.10797">arXiv</a>
                                    <p></p>
                                    <p>
                                        Explaining a deep learning model can help users understand its behavior and allow researchers to discern its shortcomings. Recent work has primarily focused on explaining models for tasks like image classification or visual question answering. In this paper, we introduce Salient Attributes for Network Explanation (SANE) to explain image similarity models, where a model‚Äôs output is a score measuring the similarity of two inputs rather than a classification score. In this task, an explanation depends on both of the input images, so standard methods do not apply. Our SANE explanations pairs a saliency map identifying important image regions with an attribute that best explains the match. We find that our explanations provide additional information not typically captured by saliency maps alone, and can also improve performance on the classic task of attribute recognition. Our approach‚Äôs ability to generalize is demonstrated on two datasets from diverse domains, Polyvore Outfits and <a href="https://cvml.ista.ac.at/AwA2/">Animals with Attributes 2</a>.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Fashion Compatibility project section -->

                    <table style="width:100%;margin:auto;border-collapse:separate;border-spacing:0;">
                        <tbody>
                            <tr onmouseout="fashion_stop()" onmouseover="fashion_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <div class="two" id='fashion_image' style="opacity: 0;">
                                            <img src='images/fashion1.png' width="160" alt="fashion image 1">
                                        </div>
                                        <img src='images/fashion2.png' width="160" alt="fashion image 2">
                                    </div>
                                    <script type="text/javascript">
                                        function fashion_start() {
                                            document.getElementById('fashion_image').style.opacity = "1";
                                        }
                                        function fashion_stop() {
                                            document.getElementById('fashion_image').style.opacity = "0";
                                        }
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/pdf/1803.09196">
                                        <span class="papertitle">
                                            Learning Type-Aware Embeddings for Fashion Compatibility
                                        </span>
                                    </a>
                                    <br>
                                    <strong>Mariya I. Vasileva</strong>,
                                    <a href="https://bryanplummer.com/">Bryan A. Plummer</a>,
                                    <a>Krishna Dusad</a>,
                                    <a href="https://shreya-rajpal.com/">Shreya Rajpal</a>,
                                    <a href="https://ranjithakumar.net/">Ranjitha Kumar</a>,
                                    <a href="http://luthuli.cs.uiuc.edu/~daf/">David A. Forsyth</a>
                                    <br>
                                    <em>ECCV</em>, 2018
                                    <br>
                                    <a href="https://github.com/mvasil/fashion-compatibility">code</a> /
                                    <a href="https://arxiv.org/abs/1803.09196">arXiv</a>
                                    <p></p>
                                    <p>
                                        Outfits in online fashion data are composed of items of many different types (e.g. top, bottom, shoes) that share some stylistic relationship with one another. A representation for building outfits requires a method that can learn both notions of similarity (for example, when two tops are interchangeable) and compatibility (items of possibly different type that can go together in an outfit). This paper presents an approach to learning an image embedding that respects item type, and jointly learns notions of item similarity and compatibility in an end-toend model. To evaluate the learned representation, we crawled 68,306 outfits created by users on the Polyvore website. Our approach obtains 3-5% improvement over the state-of-the-art on outfit compatibility prediction and fill-in-the-blank tasks using our dataset, as well as an established smaller dataset, while supporting a variety of useful queries
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- Similarity Conditions project section -->

                    <table style="width:100%;margin:auto;border-collapse:separate;border-spacing:0;">
                        <tbody>
                            <tr onmouseout="sim_stop()" onmouseover="sim_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <div class="two" id='sim_image' style="opacity: 0;">
                                            <img src='images/scenet1.png' width="160" alt="sim image 1">
                                        </div>
                                        <img src='images/scenet2.png' width="160" alt="sim image 2">
                                    </div>
                                    <script type="text/javascript">
                                        function sim_start() {
                                            document.getElementById('sim_image').style.opacity = "1";
                                        }
                                        function sim_stop() {
                                            document.getElementById('sim_image').style.opacity = "0";
                                        }
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/pdf/1908.08589">
                                        <span class="papertitle">
                                            Learning Similarity Conditions Without Explicit Supervision
                                        </span>
                                    </a>
                                    <br>
                                    <a>Reuben Tan</a>,
                                    <strong>Mariya I. Vasileva</strong>,
                                    <a href="https://bryanplummer.com/">Bryan A. Plummer</a>,
                                    <a href="https://ai.bu.edu/ksaenko.html">Kate Saenko</a>,
                                    <a href="http://luthuli.cs.uiuc.edu/~daf/">David A. Forsyth</a>
                                    <br>
                                    <em>ICCV</em>, 2019
                                    <br>
                                    <a href="https://github.com/rxtan2/Learning-Similarity-Conditions">code</a> /
                                    <a href="https://arxiv.org/abs/1908.08589">arXiv</a>
                                    <p></p>
                                    <p>
                                        Many real-world tasks require models to compare images along multiple similarity conditions (e.g. similarity in color, category or shape). Existing methods often reason about these complex similarity relationships by learning condition-aware embeddings. While such embeddings aid models in learning different notions of similarity, they also limit their capability to generalize to unseen categories since they require explicit labels at test time. To address this deficiency, we propose an approach that jointly learns representations for the different similarity conditions and their contributions as a latent variable without explicit supervision. Comprehensive experiments across three datasets, Polyvore-Outfits, Maryland-Polyvore and UT-Zappos50k, demonstrate the effectiveness of our approach: our model outperforms the state-of-the-art methods, even those that are strongly supervised with pre-defined similarity conditions, on fill-in-the-blank, outfit compatibility prediction and triplet prediction tasks. Finally, we show that our model learns different visually-relevant semantic sub-spaces that allow it to generalize well to unseen categories.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                </td>
            </tr>
        </tbody>
    </table>

</body>
</html>